{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "V3KSRJGzsfPh"
      ],
      "toc_visible": true,
      "mount_file_id": "1lkOHANi_A0f8FSEMpekKjFrML7s7FPDX",
      "authorship_tag": "ABX9TyMhmZzQh5J4wLv/ZFuzghtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucken99/ConstitutionXpert/blob/main/ProjectCI_Aug26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Install all required libraries\n",
        "# !pip install langchain\n",
        "# !pip install pypdf\n",
        "\n",
        "# !pip install pdfminer.six\n",
        "# # !pip install unstructured pdf2image  # for unstructured pdf loader (legacy)\n",
        "\n",
        "# !pip install tiktoken\n",
        "# !pip install sentence_transformers\n",
        "# !pip install chromadb\n",
        "# !pip install cohere\n",
        "# !pip install openai\n",
        "!pip install litellm"
      ],
      "metadata": {
        "id": "BgpefP-o6XBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "OjBrceNL1OZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have text file which consists of paragraphs related to Indian Constitution for e.g., Articles, Schedules, etc.\n",
        "\n",
        ">"
      ],
      "metadata": {
        "id": "9Alt6gid1UoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data path\n",
        "dir_path = \"/content/drive/MyDrive/Project_CILLM/db/\"\n",
        "text_file_path = \"/content/drive/MyDrive/Project_CILLM/db/file_context_corpus_cleaned_extended_part3.txt\"\n",
        "text_file_path = \"/content/drive/MyDrive/Project_CILLM/colab_project/Constitution-Xpert using OpenAI Embeddings (1)/file_context_corpus_cleaned_extended_part3.txt\"\n",
        "pdf_file_path = \"/content/drive/MyDrive/Project_CILLM/db/file_context_corpus_cleaned_extended_part3.pdf\"\n"
      ],
      "metadata": {
        "id": "5TlhFoKt5D3V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to load api keys from json file\n",
        "import json\n",
        "path_to_keys = \"/content/drive/MyDrive/Project_CILLM/Keys/keys.json\"\n",
        "def return_api_key(name):\n",
        "    with open(path_to_keys, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "        return json_data[name]"
      ],
      "metadata": {
        "id": "E4NvkzreAul5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVAL\n"
      ],
      "metadata": {
        "id": "lJrYlQmfbc3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)"
      ],
      "metadata": {
        "id": "pmjql4Pn6E4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Loaders\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# PDF Loaders (try which suits us best)\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "# from langchain.document_loaders import MathpixPDFLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
        "from langchain.document_loaders import AmazonTextractPDFLoader\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "loader_text = TextLoader(text_file_path)\n",
        "loader_pdf = PyPDFLoader(pdf_file_path)"
      ],
      "metadata": {
        "id": "wZdrSHYF6LPX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## utility function for document loaders information\n",
        "\n",
        "def loaded_doc_info(loader, show_loaded_data=False):\n",
        "    data = loader.load()\n",
        "    print(\"Type of the loader:\", type(loader))\n",
        "    print(\"Length of the data:\", len(data))\n",
        "    if show_loaded_data:\n",
        "        print(data)\n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xyATwh2GB2tY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_doc_info(loader_text, show_loaded_data=True)\n",
        "text_data = loaded_doc_info(loader_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uU64OAz_3YW",
        "outputId": "5d9ba8e5-8b10-4b69-eaf8-2711ba343372"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the loader: <class 'langchain.document_loaders.text.TextLoader'>\n",
            "Length of the data: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_data = loaded_doc_info(loader_pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EalxDyqJHV8h",
        "outputId": "9a02ec7b-4af3-4e9c-fcfc-2bdf6e6f9f16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the loader: <class 'langchain.document_loaders.pdf.PyPDFLoader'>\n",
            "Length of the data: 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # checking different pdf loader\n",
        "# loader = PDFMinerPDFasHTMLLoader(pdf_file_path)\n",
        "# data = loaded_doc_info(loader, True)"
      ],
      "metadata": {
        "id": "LvAi8Ws9AMHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Document Transformers](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
      ],
      "metadata": {
        "id": "QnoPoGXYFIMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter  # we can split by parts in constitution using this splitter and add metadata for better search\n",
        "                                                                # first we have to add headers (for e.g., # or ##)\n",
        "\n",
        "# # Split by tokens\n",
        "# from langchain.text_splitter import TokenTextSplitter\n",
        "# from langchain.text_splitter import SpacyTextSplitter\n",
        "# from langchain.text_splitter import NLTKTextSplitter\n",
        "\n",
        "# !pip install tiktoken\n",
        "# good for OpenAI Models\n",
        "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "#     chunk_size=100, chunk_overlap=0\n",
        "# )\n",
        "\n",
        "# # Sentence Transformers token split\n",
        "# from langchain.text_splitter import SentenceTransformersTokenTextSplitter # for a particular sentence transformer\n",
        "\n",
        "# # Hugging face tokenizers\n",
        "# from transformers import GPT2TokenizerFast\n",
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "#     tokenizer, chunk_size=100, chunk_overlap=0\n",
        "# )\n",
        "# texts = text_splitter.split_text(text)\n"
      ],
      "metadata": {
        "id": "Fmp1TBAPG6JD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(loader, splitter):\n",
        "    docs = loader.load_and_split(splitter)\n",
        "    print(\"Loader Type:\", type(loader))\n",
        "    print(\"Splitter Type:\", type(splitter))\n",
        "    print(\"Length of splitted data:\", len(docs))\n",
        "    return docs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zHGMmb19PywQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using openai's tiktoken for splitting data\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder()\n",
        "docs_text = split_data(loader_text, text_splitter)\n",
        "docs_pdf = split_data(loader_pdf, text_splitter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgKj3hfnEjOk",
        "outputId": "9dde4af0-d32b-479c-fba4-7bf12cdebc3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader Type: <class 'langchain.document_loaders.text.TextLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.CharacterTextSplitter'>\n",
            "Length of splitted data: 31\n",
            "Loader Type: <class 'langchain.document_loaders.pdf.PyPDFLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.CharacterTextSplitter'>\n",
            "Length of splitted data: 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using RecursiveCharacterTextSplitter\n",
        "recur_splitter = RecursiveCharacterTextSplitter()\n",
        "docs_text = split_data(loader_text, recur_splitter)\n",
        "docs_pdf = split_data(loader_pdf, recur_splitter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_dlvO-5RueP",
        "outputId": "8a1449de-6ff1-4faa-9421-9556bca1e0e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader Type: <class 'langchain.document_loaders.text.TextLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.RecursiveCharacterTextSplitter'>\n",
            "Length of splitted data: 156\n",
            "Loader Type: <class 'langchain.document_loaders.pdf.PyPDFLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.RecursiveCharacterTextSplitter'>\n",
            "Length of splitted data: 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_pdf[0]"
      ],
      "metadata": {
        "id": "D1Gdxg-cR0Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    # chunk_size = 1000,\n",
        "    # chunk_overlap  = 200,\n",
        "    # length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "# char_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
        "\n",
        "docs_text = split_data(loader_text, char_splitter)\n",
        "docs_pdf = split_data(loader_pdf, char_splitter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VROD2RVCT1q7",
        "outputId": "b7e49881-2206-4d3f-e58a-c4e2518215ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loader Type: <class 'langchain.document_loaders.text.TextLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.CharacterTextSplitter'>\n",
            "Length of splitted data: 156\n",
            "Loader Type: <class 'langchain.document_loaders.pdf.PyPDFLoader'>\n",
            "Splitter Type: <class 'langchain.text_splitter.CharacterTextSplitter'>\n",
            "Length of splitted data: 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [doc.page_content for doc in docs_text]"
      ],
      "metadata": {
        "id": "HBWNbzN4sD49"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XmSCpOYeWafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs_pdf[0].page_content"
      ],
      "metadata": {
        "id": "OvSaSxsDU-Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(docs_pdf[0].page_content.rstrip('\\n'))"
      ],
      "metadata": {
        "id": "zilnW8L3UeZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Embedding models and chromadb Vectorstore\n",
        "[MTEB blog](https://huggingface.co/blog/mteb) <br>\n",
        "[MTEB](https://huggingface.co/spaces/mteb/leaderboard)"
      ],
      "metadata": {
        "id": "ER6i0-85UqwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chromadb VectorStore\n"
      ],
      "metadata": {
        "id": "1WT99JNfyAq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "xRsZ-4_VyHfQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for embeddings info\n",
        "\n",
        "def embeddings_info(embeddings):\n",
        "    print(\"Total Embeddings:\", len(embeddings))\n",
        "    print(\"Dimension:\", len(embeddings[0]))\n"
      ],
      "metadata": {
        "id": "VLKfVutytshL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Problems with Chroma\n",
        "### https://python.langchain.com/docs/integrations/vectorstores/chroma#basic-example-including-saving-to-disk\n",
        "## Caching Embeddings using LocalFileStore\n",
        "# from langchain.storage import LocalFileStore\n",
        "# from langchain.embeddings import CacheBackedEmbeddings # Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n",
        "# import os\n",
        "\n",
        "# utility function for creating embeddings using chromdb\n",
        "# def create_cached_embeddings(docs, embedding_model, model_name, fs):\n",
        "#     # print(list(fs.yield_keys()))\n",
        "#     # cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "#     #     embedding_model, fs, namespace=model_name\n",
        "#     # )\n",
        "#     ### Create the vectorstore\n",
        "#     # db = Chroma.from_documents(docs, cached_embedder)\n",
        "#     db = Chroma.from_documents(docs, embedding_model)\n",
        "#     return db\n",
        "\n",
        "# def create_embeddings(docs, embedding_model):\n",
        "#     return Chroma.from_documents(docs, embedding_model)\n",
        "\n",
        "\n",
        "# utility function for similarity search\n",
        "def return_similar_docs(db, query, k=4, show_docs=False):\n",
        "    docs = db.similarity_search_with_relevance_scores(query, k=k)\n",
        "    docs = sorted(docs, key=lambda x: -x[1])\n",
        "    if show_docs:\n",
        "        for doc in docs:\n",
        "            print(\"Text:\", doc[0].page_content)\n",
        "            print(\"Relevance Score:\", doc[1])\n",
        "            print(\"--\"*50)\n",
        "            print(\"--\"*50)\n",
        "    return docs\n",
        "\n"
      ],
      "metadata": {
        "id": "zMLnXd4x3KOz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BGE Hugging face embeddings"
      ],
      "metadata": {
        "id": "HUNJ2TN-jhkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "LOtmP_kUmaq6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings = hf.embed_documents(docs)\n",
        "# embeddings_info(embeddings)\n",
        "# sentence1 = \"Tell me about article 23\"\n",
        "# embedding1 = hf.embed_query(sentence1)\n",
        "# import numpy as np\n",
        "\n",
        "# dot_product = np.dot(embeddings, embedding1)\n",
        "# dot_product\n",
        "# np.argmax()"
      ],
      "metadata": {
        "id": "YUX28y8VpMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hf_db = Chroma.from_documents(docs_text, hf, persist_directory=\"./chroma_db_hf\")\n",
        "hf_db = Chroma(persist_directory=\"./chroma_db_hf\", embedding_function=hf)"
      ],
      "metadata": {
        "id": "jzCQlsvX0FVi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### check similar documents\n",
        "query = \"What is writs in constitution?\"\n",
        "retrived_docs = return_similar_docs(hf_db, query, show_docs=True)"
      ],
      "metadata": {
        "id": "FahmaWQ0uOdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cohere Embeddings"
      ],
      "metadata": {
        "id": "j7DDvAbVwDLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COHERE_API_KEY = return_api_key(\"COHERE_API_KEY\")"
      ],
      "metadata": {
        "id": "t9yI8u4TCWTl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import CohereEmbeddings\n",
        "cohr = CohereEmbeddings(\n",
        "    model=\"embed-english-v2.0\",\n",
        "    cohere_api_key=COHERE_API_KEY\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lXyxhF_CwT9Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db_cohr = create_embeddings(docs_text, cohr)\n",
        "# cohr_db = Chroma.from_documents(docs_text, cohr, persist_directory=\"./chroma_db_cohr\")\n",
        "cohr_db = Chroma(persist_directory=\"./chroma_db_cohr\", embedding_function=cohr)"
      ],
      "metadata": {
        "id": "B3D2GhCdwo6B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### check similar documents\n",
        "query = \"What is writs in constitution?\"\n",
        "retrieved_docs = return_similar_docs(cohr_db, query, show_docs=True)"
      ],
      "metadata": {
        "id": "fqZWPr-zGWiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reordering\n",
        "- When models must access relevant information in the middle of long contexts, then tend to ignore the provided documents. See: https://arxiv.org/abs/2307.03172\n"
      ],
      "metadata": {
        "id": "qcOReyx7c9eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "\n",
        "# Reorder returned docs\n",
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(retrieved_docs)\n",
        "reordered_docs"
      ],
      "metadata": {
        "id": "41Yhjut9dUE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Embeddings"
      ],
      "metadata": {
        "id": "-1eyV80gn9sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = return_api_key(\"OPENAI_API_KEY\")\n",
        "# OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "WdW32-0prwtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "openai = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "A3HPSd4vr360"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can also use [VectorstoreIndexCreator](https://python.langchain.com/docs/modules/data_connection/retrievers/#one-line-index-creation)\n",
        "for creating vectorstore quickly with one liner code."
      ],
      "metadata": {
        "id": "8XznW6gTElyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "index_creator = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=Chroma,\n",
        "    embedding=CohereEmbeddings(cohere_api_key=COHERE_API_KEY),\n",
        "    text_splitter=CharacterTextSplitter(separator='\\n\\n'),\n",
        ")\n",
        "\n",
        "index = index_creator.from_loaders([loader_text])\n",
        "\n",
        "# querying\n",
        "query = \"what is article 4?\"\n",
        "# index.query(query) # give your llm\n",
        "# index.query_with_sources(query) # give your llm\n",
        "\n",
        "# check Vectorstore\n",
        "index.vectorstore\n",
        "\n",
        "# can use as a retriever\n",
        "index.vectorstore.as_retriever()\n",
        "\n",
        "# can use in QA using llm\n",
        "# qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcsbPVtvE4Ht",
        "outputId": "df481e4e-5ef6-4773-9bfb-72ac54a65fb7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x78572fc8a650>, search_type='similarity', search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
      ],
      "metadata": {
        "id": "d6AzTQMZsNpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [MultiQueryRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)"
      ],
      "metadata": {
        "id": "V3KSRJGzsfPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using cohere llm via LiteLLM lib interface\n",
        "# from langchain.chat_models import ChatLiteLLM\n",
        "from langchain.llms import Cohere\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "# llm = ChatLiteLLM(model=\"command-nightly\", cohere_api_key=COHERE_API_KEY)\n",
        "llm = Cohere(model=\"command-nightly\", cohere_api_key=COHERE_API_KEY)\n",
        "# llm.cohere_api_key\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=cohr_db.as_retriever(), llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "v4oJcuzLC82h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n"
      ],
      "metadata": {
        "id": "Y4DPDEaROJrD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem with the litellm lib\n",
        "question = \"Tell me about article 4\"\n",
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "len(unique_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vSP7YiEOt66",
        "outputId": "50c52143-b3bf-4e6c-c3c1-b3c2eec69e2a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Tell me about article 4's tags.\", \"2. Tell me about article 4's content.\", \"3. Tell me about article 4's authors.\"]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can supply our own prompt along with [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/) to split the results into a list of queries\n",
        "\n"
      ],
      "metadata": {
        "id": "IdYvXQbZPBAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain import LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions seperated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "# Chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
        "\n",
        "# Other inputs\n",
        "question = \"what is reservation in contitution of india?\""
      ],
      "metadata": {
        "id": "d8s0JEM0aMq4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=cohr_db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
        ")  # \"lines\" is the key (attribute name) of the parsed output\n",
        "\n",
        "# Results\n",
        "unique_docs = retriever.get_relevant_documents(\n",
        "    query=question\n",
        ")\n",
        "len(unique_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU3RZvIpa0rh",
        "outputId": "c80e0024-8528-4869-eace-6025ab2ddf09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the reservation in the Constitution of India according to the Constitution?', '2. What does the Constitution of India say about reservation?', '3. How does the reservation in the Constitution of India work?', '4. What are the different types of reservation in the Constitution of India?', '5. How has the reservation in the Constitution of India changed over time?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTz8YKKQa9on"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4XNHlTre95U"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3W40nmke98l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYtVbxf9e9_h"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Ensemble Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)\n",
        "- The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm."
      ],
      "metadata": {
        "id": "vJzsgqNKe-Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever"
      ],
      "metadata": {
        "id": "TCNDL9k8iH8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiVector Retriever"
      ],
      "metadata": {
        "id": "XvZQNMW8f8lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Querying"
      ],
      "metadata": {
        "id": "0pssBSF3fPxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WebResearchRetriever"
      ],
      "metadata": {
        "id": "5I0LClgGe-Ev"
      }
    }
  ]
}